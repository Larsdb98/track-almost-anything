{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 Object Detection\n",
    "\n",
    "Testing out detection algorithms to implement. \n",
    "\n",
    "### Dependencies and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from track_almost_anything.api.processing.utils import TorchBackend\n",
    "from track_almost_anything.api.io.image_io import get_image_sequence_config_from_dir\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load an image using OpenCV.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    # Convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_yolo_model(device):\n",
    "    \"\"\"Load a pretrained YOLOv5 model.\"\"\"\n",
    "    # Load YOLOv5s (small) model\n",
    "    # model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/resources/detection_models/yolov5s.pt\")\n",
    "    # model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/resources/detection_models/yolov5s.pt\") # YOLOv5 S\n",
    "    # model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/resources/detection_models/yolov5m.pt\") # YOLOv5 M\n",
    "    # model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/resources/detection_models/yolov5l.pt\") # YOLOv5 L\n",
    "    model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/resources/detection_models/yolov5x.pt\") # YOLOv5 X\n",
    "    # model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5x\")\n",
    "\n",
    "    # Testing model loading directly with torch.load():\n",
    "    \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def detect_objects(model, image_path):\n",
    "    \"\"\"Detect objects using YOLOv5.\"\"\"\n",
    "    # Perform inference\n",
    "    results = model(image_path)\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_results(image, results):\n",
    "    \"\"\"Visualize detected objects.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    detections = results.xyxy[0].cpu().numpy()  # Get detections for the first image\n",
    "    for *box, conf, cls in detections:\n",
    "        x1, y1, x2, y2 = box\n",
    "        conf = float(conf)\n",
    "        cls = int(cls)\n",
    "        label = results.names[cls]\n",
    "\n",
    "        # Draw the bounding box\n",
    "        rect = Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add the label and confidence score\n",
    "        label_text = f\"{label}: {conf:.2f}\"\n",
    "        ax.text(\n",
    "            x1,\n",
    "            y1 - 5,\n",
    "            label_text,\n",
    "            color=\"white\",\n",
    "            fontsize=10,\n",
    "            bbox=dict(facecolor=\"red\", alpha=0.5),\n",
    "        )\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_backend = TorchBackend()\n",
    "device = torch_backend.get()\n",
    "# torch.hub.set_dir(\"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/resources/detection_models/\")\n",
    "\n",
    "image_path = \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything_resources/Video/MOT17/test/MOT17-08-DPM/img1/000004.jpg\"\n",
    "out_path = \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything_resources/detections.png\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "model = get_yolo_model(device)\n",
    "\n",
    "print(\"Starting inference..\")\n",
    "results = detect_objects(model, image_path)\n",
    "print(\"Results:\")\n",
    "print(results)\n",
    "\n",
    "visualize_results(image, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolov8 With Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from track_almost_anything.api.processing.utils import TorchBackend\n",
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load an image using OpenCV.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    # Convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def detect_objects(model, image_path):\n",
    "    \"\"\"Detect objects using YOLOv5.\"\"\"\n",
    "    # Perform inference\n",
    "    results = model(image_path)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything/research/detection\n",
      "Starting inference..\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 159.2ms\n",
      "Speed: 2.8ms preprocess, 159.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results:\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[244, 231, 214],\n",
      "        [217, 204, 185],\n",
      "        [169, 157, 135],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[251, 238, 221],\n",
      "        [201, 188, 169],\n",
      "        [146, 134, 112],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[246, 233, 216],\n",
      "        [190, 177, 158],\n",
      "        [145, 133, 111],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 79,  93, 102],\n",
      "        [ 79,  93, 102],\n",
      "        [ 78,  92, 101],\n",
      "        ...,\n",
      "        [106,  95,  91],\n",
      "        [106,  95,  91],\n",
      "        [106,  95,  91]],\n",
      "\n",
      "       [[ 80,  94, 103],\n",
      "        [ 79,  93, 102],\n",
      "        [ 79,  93, 102],\n",
      "        ...,\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92]],\n",
      "\n",
      "       [[ 80,  94, 103],\n",
      "        [ 80,  94, 103],\n",
      "        [ 79,  93, 102],\n",
      "        ...,\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict'\n",
      "speed: {'preprocess': 2.7959346771240234, 'inference': 159.1942310333252, 'postprocess': 1.1179447174072266}]\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9118])\n",
      "data: tensor([[1.6000e+03, 3.5289e+02, 1.9188e+03, 1.0720e+03, 9.1177e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[1759.3855,  712.4366,  318.8185,  719.0919]])\n",
      "xywhn: tensor([[0.9163, 0.6597, 0.1661, 0.6658]])\n",
      "xyxy: tensor([[1599.9763,  352.8907, 1918.7948, 1071.9825]])\n",
      "xyxyn: tensor([[0.8333, 0.3268, 0.9994, 0.9926]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8985])\n",
      "data: tensor([[1.0723e+02, 3.0965e+02, 4.8046e+02, 1.0722e+03, 8.9851e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[293.8425, 690.9258, 373.2274, 762.5493]])\n",
      "xywhn: tensor([[0.1530, 0.6397, 0.1944, 0.7061]])\n",
      "xyxy: tensor([[ 107.2288,  309.6511,  480.4562, 1072.2004]])\n",
      "xyxyn: tensor([[0.0558, 0.2867, 0.2502, 0.9928]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8650])\n",
      "data: tensor([[5.8090e+02, 2.8529e+02, 8.4898e+02, 1.0601e+03, 8.6499e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[714.9403, 672.6799, 268.0801, 774.7759]])\n",
      "xywhn: tensor([[0.3724, 0.6229, 0.1396, 0.7174]])\n",
      "xyxy: tensor([[ 580.9003,  285.2920,  848.9804, 1060.0679]])\n",
      "xyxyn: tensor([[0.3026, 0.2642, 0.4422, 0.9815]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7519])\n",
      "data: tensor([[1.5675e+03, 3.6416e+02, 1.6520e+03, 6.1035e+02, 7.5193e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[1609.7435,  487.2548,   84.4556,  246.1891]])\n",
      "xywhn: tensor([[0.8384, 0.4512, 0.0440, 0.2280]])\n",
      "xyxy: tensor([[1567.5157,  364.1602, 1651.9713,  610.3493]])\n",
      "xyxyn: tensor([[0.8164, 0.3372, 0.8604, 0.5651]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7076])\n",
      "data: tensor([[8.7949e+02, 3.7311e+02, 9.3544e+02, 5.7574e+02, 7.0761e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[907.4678, 474.4270,  55.9517, 202.6279]])\n",
      "xywhn: tensor([[0.4726, 0.4393, 0.0291, 0.1876]])\n",
      "xyxy: tensor([[879.4919, 373.1131, 935.4436, 575.7410]])\n",
      "xyxyn: tensor([[0.4581, 0.3455, 0.4872, 0.5331]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7067])\n",
      "data: tensor([[9.5627e+02, 3.5298e+02, 1.0873e+03, 5.9859e+02, 7.0668e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[1021.7827,  475.7863,  131.0233,  245.6072]])\n",
      "xywhn: tensor([[0.5322, 0.4405, 0.0682, 0.2274]])\n",
      "xyxy: tensor([[ 956.2711,  352.9827, 1087.2943,  598.5899]])\n",
      "xyxyn: tensor([[0.4981, 0.3268, 0.5663, 0.5542]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6494])\n",
      "data: tensor([[1.6394e+03, 3.4411e+02, 1.7105e+03, 5.5315e+02, 6.4945e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[1674.9619,  448.6306,   71.1097,  209.0335]])\n",
      "xywhn: tensor([[0.8724, 0.4154, 0.0370, 0.1935]])\n",
      "xyxy: tensor([[1639.4070,  344.1138, 1710.5167,  553.1473]])\n",
      "xyxyn: tensor([[0.8539, 0.3186, 0.8909, 0.5122]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([26.])\n",
      "conf: tensor([0.6005])\n",
      "data: tensor([[4.0166e+02, 6.7674e+02, 5.6848e+02, 9.6297e+02, 6.0047e-01, 2.6000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[485.0676, 819.8574, 166.8248, 286.2330]])\n",
      "xywhn: tensor([[0.2526, 0.7591, 0.0869, 0.2650]])\n",
      "xyxy: tensor([[401.6552, 676.7408, 568.4800, 962.9739]])\n",
      "xyxyn: tensor([[0.2092, 0.6266, 0.2961, 0.8916]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([26.])\n",
      "conf: tensor([0.5379])\n",
      "data: tensor([[7.8405e+02, 6.9531e+02, 8.8450e+02, 9.6177e+02, 5.3790e-01, 2.6000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[834.2739, 828.5385, 100.4531, 266.4532]])\n",
      "xywhn: tensor([[0.4345, 0.7672, 0.0523, 0.2467]])\n",
      "xyxy: tensor([[784.0474, 695.3119, 884.5005, 961.7651]])\n",
      "xyxyn: tensor([[0.4084, 0.6438, 0.4607, 0.8905]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5222])\n",
      "data: tensor([[8.3463e+01, 3.7726e+02, 1.8667e+02, 6.3234e+02, 5.2222e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[135.0651, 504.8022, 103.2034, 255.0819]])\n",
      "xywhn: tensor([[0.0703, 0.4674, 0.0538, 0.2362]])\n",
      "xyxy: tensor([[ 83.4634, 377.2612, 186.6668, 632.3431]])\n",
      "xyxyn: tensor([[0.0435, 0.3493, 0.0972, 0.5855]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4819])\n",
      "data: tensor([[3.7757e+01, 3.8709e+02, 1.1355e+02, 6.3384e+02, 4.8193e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.6527, 510.4668,  75.7909, 246.7456]])\n",
      "xywhn: tensor([[0.0394, 0.4727, 0.0395, 0.2285]])\n",
      "xyxy: tensor([[ 37.7573, 387.0940, 113.5482, 633.8396]])\n",
      "xyxyn: tensor([[0.0197, 0.3584, 0.0591, 0.5869]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3562])\n",
      "data: tensor([[9.2703e+00, 3.8762e+02, 5.5958e+01, 5.2982e+02, 3.5617e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 32.6139, 458.7163,  46.6872, 142.1990]])\n",
      "xywhn: tensor([[0.0170, 0.4247, 0.0243, 0.1317]])\n",
      "xyxy: tensor([[  9.2703, 387.6168,  55.9575, 529.8159]])\n",
      "xyxyn: tensor([[0.0048, 0.3589, 0.0291, 0.4906]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([26.])\n",
      "conf: tensor([0.3269])\n",
      "data: tensor([[5.5198e+02, 7.3567e+02, 6.4125e+02, 9.2555e+02, 3.2693e-01, 2.6000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[596.6153, 830.6094,  89.2716, 189.8807]])\n",
      "xywhn: tensor([[0.3107, 0.7691, 0.0465, 0.1758]])\n",
      "xyxy: tensor([[551.9795, 735.6691, 641.2511, 925.5498]])\n",
      "xyxyn: tensor([[0.2875, 0.6812, 0.3340, 0.8570]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3130])\n",
      "data: tensor([[8.2834e+02, 3.7335e+02, 8.7993e+02, 5.4906e+02, 3.1302e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[854.1342, 461.2009,  51.5942, 175.7102]])\n",
      "xywhn: tensor([[0.4449, 0.4270, 0.0269, 0.1627]])\n",
      "xyxy: tensor([[828.3371, 373.3458, 879.9313, 549.0560]])\n",
      "xyxyn: tensor([[0.4314, 0.3457, 0.4583, 0.5084]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3072])\n",
      "data: tensor([[9.4695e+02, 3.6114e+02, 9.8850e+02, 5.4054e+02, 3.0718e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[967.7242, 450.8401,  41.5562, 179.4095]])\n",
      "xywhn: tensor([[0.5040, 0.4174, 0.0216, 0.1661]])\n",
      "xyxy: tensor([[946.9461, 361.1353, 988.5023, 540.5448]])\n",
      "xyxyn: tensor([[0.4932, 0.3344, 0.5148, 0.5005]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything_resources/Video/MOT17/test/MOT17-08-DPM/img1/000004.jpg\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "torch_backend = TorchBackend()\n",
    "device = torch_backend.get()\n",
    "print(os.getcwd())\n",
    "\n",
    "model = YOLO(\"../../resources/detection_models/yolov8n.pt\")\n",
    "# model = YOLO(\"yolov5x.pt\")\n",
    "model.model.to(device)\n",
    "\n",
    "print(\"Starting inference..\")\n",
    "results = detect_objects(model, image)\n",
    "print(\"Results:\")\n",
    "print(results)\n",
    "\n",
    "\n",
    "detections = results[0].cpu()\n",
    "for detections in detections.boxes:\n",
    "    print(detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 12 persons, 3 handbags, 234.4ms\n",
      "Speed: 3.8ms preprocess, 234.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[244, 231, 214],\n",
      "        [217, 204, 185],\n",
      "        [169, 157, 135],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[251, 238, 221],\n",
      "        [201, 188, 169],\n",
      "        [146, 134, 112],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[246, 233, 216],\n",
      "        [190, 177, 158],\n",
      "        [145, 133, 111],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 79,  93, 102],\n",
      "        [ 79,  93, 102],\n",
      "        [ 78,  92, 101],\n",
      "        ...,\n",
      "        [106,  95,  91],\n",
      "        [106,  95,  91],\n",
      "        [106,  95,  91]],\n",
      "\n",
      "       [[ 80,  94, 103],\n",
      "        [ 79,  93, 102],\n",
      "        [ 79,  93, 102],\n",
      "        ...,\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92]],\n",
      "\n",
      "       [[ 80,  94, 103],\n",
      "        [ 80,  94, 103],\n",
      "        [ 79,  93, 102],\n",
      "        ...,\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92],\n",
      "        [107,  96,  92]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict'\n",
      "speed: {'preprocess': 3.787994384765625, 'inference': 234.42506790161133, 'postprocess': 1.6639232635498047}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "from track_almost_anything.api.processing.detection import ObjectDetection\n",
    "from track_almost_anything.api.processing.utils import TorchBackend\n",
    "import cv2\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load an image using OpenCV.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    # Convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "backend = TorchBackend()\n",
    "device = backend.get()\n",
    "detector = ObjectDetection(detection_family=\"yolov8\", model_size=\"n\", device=device)\n",
    "\n",
    "image_path = \"/Users/larsdelbubba/Desktop/Coding Projects/track-almost-anything_resources/Video/MOT17/test/MOT17-08-DPM/img1/000004.jpg\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "results = detector.predict(image=image)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track-almost-anything-iVCq83Ic-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
